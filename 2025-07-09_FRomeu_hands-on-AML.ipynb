{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1908a0b",
   "metadata": {},
   "source": [
    "# Practical session on Adversarial Machine Learning\n",
    "**Summer School Cyber in Font-Romeu**\n",
    "\n",
    "July 9th 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b3ee0",
   "metadata": {},
   "source": [
    "### Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4fa58f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "# For some reason, we need to run this\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, BoundaryAttack\n",
    "from art.defences.detector.evasion import BinaryInputDetector\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from utils import random_subsample, show_images, plot_images_pca, create_model_cifar, load_cifar100_file, plot_adversarial_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f5785fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASS=b'people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b25fd185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81a9d4",
   "metadata": {},
   "source": [
    "### Adversarial machine learning: the science of fooling machine learning models.\n",
    "\n",
    "The two main aspects of adversarial machine learning are:\n",
    "1. **Attacks:** Techniques to generate inputs that cause a model to make wrong predictions with high confidence. These can be:\n",
    "    - Evasion attacks (modifying inputs at test time, e.g., adding small perturbations to images to fool a classifier),\n",
    "    - Poisoning attacks (manipulating the training data to embed vulnerabilities),\n",
    "    - Exploratory attacks (extracting information about the model, e.g., through model inversion or membership inference).\n",
    "2. **Defenses:** Strategies to make models more robust or to detect threats. Examples include\n",
    "    - adversarial training (training on perturbed data),\n",
    "    - detection of poisoning and adversarial examples,\n",
    "    - certified robustness approaches.\n",
    "\n",
    "Note that poisoning, membership inference and the like where explained by Josep Domingo Ferrer on monday.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Trusted-AI/adversarial-robustness-toolbox/refs/heads/main/docs/images/white_hat_blue_red.png\" alt=\"Red and Blue teams for Adversarial ML\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491d10d",
   "metadata": {},
   "source": [
    "In this session, we will use the Adversarial Robustness Toolbox (ART) developed by IBM: https://github.com/Trusted-AI/adversarial-robustness-toolbox\n",
    "- It supports multiple attack and defense algorithms,\n",
    "- It is actively maintained and well-documented,\n",
    "- It works with major deep learning frameworks (TensorFlow, PyTorch, Keras).\n",
    "\n",
    "*Comparative table of the main adversarial ML libraries*\n",
    "| Library | Supported Frameworks | Attacks & Defenses | Ease of Use | Actively | Maintained |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Adversarial Robustness Toolbox (ART) | TensorFlow, PyTorch, Keras, MXNet | Many (FGSM, PGD, CW, etc.) | Many | High | ✅|\n",
    "| CleverHans | TensorFlow, PyTorch | Evasion (many), less focus on defenses | Few | Moderate | ⚠️ (less active)|\n",
    "| Foolbox | TensorFlow, PyTorch | Wide variety (esp. evasion) | Limited | High | ✅|\n",
    "| Advertorch | PyTorch | Evasion-focused | Some defenses | High | ⚠️ (slightly less active)|\n",
    "| DeepRobust | PyTorch | Evasion, poisoning | Several | Moderate | ✅ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc15113",
   "metadata": {},
   "source": [
    "# Part 1: The classification of images with a class \"people\"\n",
    "\n",
    "## 1.1 Dataset: CIFAR100\n",
    "\n",
    "We use here the CIFAR-100 dataset, a well-known benchmark dataset for image classification tasks.\n",
    "\n",
    "*What is in CIFAR-100?*\n",
    "- It contains 60,000 color images of size 32×32 pixels.\n",
    "- Each image belongs to one of 100 classes (for example, apple, castle, lion).\n",
    "- These 100 fine classes are grouped into 20 super classes, which represent more general categories (for example, food containers, large carnivores, people).\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240522153720/download.png\" alt=\"CIFAR100\" width = 250/>\n",
    "\n",
    "The dataset is split as follows:\n",
    "- 50,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "*Focus on the “people” class*\n",
    "\n",
    "Among the 20 super classes, there is a group called “people”, which gathers all images containing humans.\n",
    "\n",
    "In this session, we will focus on attacking this \"people\" class.\n",
    "Our goal is to study how adversarial examples can mislead a model into misclassifying images of people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972548d",
   "metadata": {},
   "source": [
    "### Load raw dataset\n",
    "- The dataset can be loaded using the *load_cifar100_file()* function from *utils.py*, see its perfectly written docstring.\n",
    "- you should obtain at least *x_train, y_train, x_test, y_test*: the training/test data x and labels y.\n",
    "- *classes* gives you the different classes for the dataset with the 20 super classes.\n",
    "- **In order to have more efficient models, you should change the classes in order to have only people vs. things.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d8c89b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e32f5",
   "metadata": {},
   "source": [
    "### Balance the dataset for training\n",
    "- **restore the balance**: If you have changed to \"people\" or \"things\", the instances of each class are now unbalanced. Randomly select a subset of the \"things\" examples of the same size as the \"people\".\n",
    "- Having balanced datasets for training is crucial, else you are building an algorithmic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e6cd5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca512d9e",
   "metadata": {},
   "source": [
    "<!-- - Optionnaly: you can select a random subset to decrease the size of the training and test datasets. You can use random_subsample from utils... But everything is fine. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "37a5026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training examples: 840\n",
      "Number of Test examples: 214\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Training examples: {len(y_train)}\")\n",
    "print(f\"Number of Test examples: {len(y_test)}\")\n",
    "print(f\"Number of classes: {len(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb5069",
   "metadata": {},
   "source": [
    "### Visualize some images\n",
    "One can use the show_images function from utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0a769f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b471b65",
   "metadata": {},
   "source": [
    "### Visualize the dataset\n",
    "- use dimensional reduction with PCA and plot the pictures in 2D\n",
    "- plot the distribution of classes in the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7116ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f368dc0",
   "metadata": {},
   "source": [
    "## 1.2 Our classification model: a classical CNN\n",
    "We use a simple Convolutional Neural Network (CNN) architecture with:\n",
    "- 3 convolutional blocks (each: convolution, max-pooling, dropout)\n",
    "- 2 dense layers (one hidden layer + final output layer)\n",
    "\n",
    "CNNs are the standard choice for image analysis, as they efficiently capture spatial patterns.\n",
    "Here, a small architecture is sufficient since our images are small (32×32) and we only have 2 classes (\"people\" and \"things\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3c99d",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "- Study the function create_model_cifar from utils.py\n",
    "- Use it to create your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a64a23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e5fb2",
   "metadata": {},
   "source": [
    "### Fine-tune the model for the particular data\n",
    "- This can be done with a simple call to the *fit* method of the model, look for the documentation of Tensorflow or examples: https://www.geeksforgeeks.org/deep-learning/model-fit-in-tensorflow/.\n",
    "- Note that the images are already normalized: pixel values are between 0 and 1 so no preprocessing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "22524508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082d879",
   "metadata": {},
   "source": [
    "## 1.3 Finally, let's start using ART ! Adversarial-Robustness Toolbox\n",
    "A key idea behind ART is that it adopts an agnostic approach with respect to the underlying machine learning library.\n",
    "This means ART is designed to work seamlessly with models implemented in different frameworks (e.g. Tensorflow, Pytorch, Scikit-learn)\n",
    "\n",
    "*Why do we wrap models?*: In our case (using TensorFlow 2.x and Keras), we use:  **from art.estimators.classification import TensorFlowV2Classifier**. This wrapper converts our TensorFlow model into a generic ART classifier, which provides a unified API for: training, evaluation, adversarial example generation, defense setup, etc.\n",
    "\n",
    "*Remark:* We could have chosen to wrap the model first, and then do the fine-tuning on the ART classifier object. In fact, we will now check the accuracy of the fine-tuned model using this interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14518bb",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model on the train and test set\n",
    "- By applying the *predict* method of the classifier, and comparing with the truth for all the examples, you can get the overall accuracy for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f4cb6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeea5ff",
   "metadata": {},
   "source": [
    "- Compute the confusion matrix for the test set (you can use *scikit-learn.metrics*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "76fd8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7010c7",
   "metadata": {},
   "source": [
    "- Test the model on a picture of your choice (resize it to 32x32, /255) and/or images from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2b3af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85c727",
   "metadata": {},
   "source": [
    "# Part 2: Attack on machines\n",
    "In this part, we focus on **evasion attacks**, where the attacker modifies inputs at test time to fool the classifier:\n",
    "- **FGSM**, **PGD**: **white-box attacks**, where the attacker has full knowledge of the model.\n",
    "- **Boundary Attack**: **black-box attack**, where the attacker can only query the model without knowing its details.\n",
    "\n",
    "Here, we show but a few example attacks. Many more are available:\n",
    "[ART attacks list](https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf143c5",
   "metadata": {},
   "source": [
    "## 2.1 FGSM (Fast Gradient Sign Method)\n",
    "One of the simplest and most popular adversarial attack techniques. \n",
    "The idea is to create a small perturbation that pushes the input in the direction that maximally increases the model's loss.\n",
    "\n",
    "The resulting perturbation is very small (almost imperceptible to the human eye), but can significantly change the model's prediction.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1043/1*7BgNv9c2n7KNq3rrUcfZ8A.png\" alt=\"Red and Blue teams for Adversarial ML\" width = 500/>\n",
    "\n",
    "See [Goodfellow et al., 2015](https://arxiv.org/abs/1412.6572)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62001536",
   "metadata": {},
   "source": [
    "### Generation of adversarial examples\n",
    "In order to apply FGSM in ART (see notebooks/adversarial_training_mnist.ipynb for an example):\n",
    "- create an object *FastGradientMethod*\n",
    "- use its method *generate* for the genration of the adversarial examples based on the test set\n",
    "- visualize the original image with its adversarial counterpart and the noise. One can use *plot_adversarial_comparison* from utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "3061d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898d23",
   "metadata": {},
   "source": [
    "### Impact on the model accuracy\n",
    "- compute the accuracy on the model for the original test set and the adversarial examples.\n",
    "- try to have the parameter epsilon vary from 0 to 0.2, plot the impact on these accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "213382b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f287c8",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- How doeas the accuracy evolve with epsilon ?\n",
    "- For the attacker, what is the risk of using high values for epsilon ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d4ba9",
   "metadata": {},
   "source": [
    "## 2.2 Projected Gradient Descent (PGD)\n",
    "The **Projected Gradient Descent (PGD)** attack is one of the most powerful and widely used to evaluate model robustness.\n",
    "\n",
    "PGD can be seen as a stronger, multi-step version of FGSM. Instead of applying a single large gradient step, PGD takes multiple small steps, each time moving slightly in the direction that increases the loss. After each step, the perturbed image is **projected back** into a valid range around the original image, to keep the perturbation small.\n",
    "\n",
    "By iterating and projecting, PGD finds more effective adversarial examples than single-step methods like FGSM.\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ4RpYZnPeRzILqJAA1wnPXk8361OW6lpukcA&s\" alt=\"Red and Blue teams for Adversarial ML\" width = 500/>\n",
    "\n",
    "See [Madry et al., 2018](https://arxiv.org/pdf/1706.06083)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aea52d",
   "metadata": {},
   "source": [
    "### Generation and test\n",
    "- Do the same as FSGM !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c48ee4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315db17b",
   "metadata": {},
   "source": [
    "###  Generate targetted adversarial examples\n",
    "It is possible to specify what class you want to be predicted for the adversarial example: targetted adversarial examples.\n",
    "In terms of the algorithm, we follow the inverse direction of the gradient, which will decrease the loss for the targeted label.\n",
    "This is close to the classical gradient descent for training.\n",
    "\n",
    "Generat target adversarial examples:\n",
    "- This can be done with\n",
    "    - the argument *targeted=True* in *ProjectedGradientDescent*\n",
    "    - the argument *y* in *generate* which gives the wanted label(s)\n",
    "- Compute the predictions for the entire adversarial dataset and display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e1662345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47915c5",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Does the confusion matrix reflect the effect of the target adversarial generation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94051321",
   "metadata": {},
   "source": [
    "## 2.3 Black Box: Boundary Attack \n",
    "This is a black-box evasion attack: this attack **only needs to query the model and observe the output label**. We do not need to know the model's parameters or architecture.\n",
    "\n",
    "The idea is to start from a **large initial adversarial example** (e.g., pure noise or an image that is already misclassified) and then **iteratively reduce the perturbation** while keeping the image adversarial.\n",
    "The algorithm moves toward the original image while staying on the \"wrong side\" of the decision boundary. This way, it progressively finds a minimally perturbed adversarial example.\n",
    "\n",
    "The attack can also be **targeted**, meaning it aims to change the prediction to a specific target class instead of just any incorrect class.\n",
    "\n",
    "See a [nice explanation here](https://github.com/greentfrapp/boundary-attack) and the original paper [Brendel et al. 2018](https://arxiv.org/abs/1712.04248).\n",
    "\n",
    "### Setting up the attack\n",
    "It is a bit complicated to set up.\n",
    "If you still have a lot of time, I advise looking at \"notebooks/attack_decision_based_boundary.ipynb\".\n",
    "Else, you can get to the next part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2b9e2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image_ind = 171\n",
    "target_image_ind = 735\n",
    "init_image=np.array([x_train[init_image_ind]])\n",
    "target_image=np.array([x_train[target_image_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5dc7457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae058f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7a56c3",
   "metadata": {},
   "source": [
    "# Part 3: Defending againt the machine\n",
    "- We start with **detection**, which aims to identify whether an input is adversarial or clean before making a prediction.\n",
    "- Then, we explore **adversarial training**, where the model is trained on adversarial examples to improve its robustness.\n",
    "\n",
    "Again, these are just a few example approaches, but other defense methods are available in ART:\n",
    "[ART defenses list](https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Defences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a377838",
   "metadata": {},
   "source": [
    "## 3.1 Detector based on the inputs\n",
    "Detecting adversarial examples can be formulated as a **binary classification problem**, where the goal is to distinguish between **legitimate** and **adversarial** inputs.  \n",
    "\n",
    "To build a detector, we can **generate adversarial examples from the training set** and label them as \"adversarial,\" while keeping the original samples labeled as \"legitimate.\"  \n",
    "\n",
    "We then train a separate **CNN-based classifier** on this combined dataset to learn to identify adversarial perturbations directly from the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65f75e",
   "metadata": {},
   "source": [
    "### Set up the Detector\n",
    "- Here, we will use a CNN which can be created with the function *create_model_cifar* from utils as in Part I.\n",
    "- This model is wrapped in ART using *TensorFlowV2Classifier* as before\n",
    "- Then, create a *BinaryInputDetector* based on this model, this is the object we use to detect adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "5f76f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9ffd6",
   "metadata": {},
   "source": [
    "### Creating adversarial examples\n",
    "Now you can create an attacker (e.g. *FastGradientMethod*) as before and generate adversarial examples on:\n",
    "- the test dataset,\n",
    "- the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "110594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bae90a",
   "metadata": {},
   "source": [
    "### The actual detector\n",
    "Now we can train and evaluate the detector model:\n",
    "- concatenate the original train dataset (label legit/0) and the corresponding generated dataset (label adversarial/1)\n",
    "- fit the detector based on the resulting training dataset\n",
    "- compute the accuracy of detection on the test set (should be legit), and on the adverarial examples generated from the test set (should be adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8326af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd0d7d",
   "metadata": {},
   "source": [
    "### Impact of epsilon\n",
    "Without retraining your detection model:\n",
    "- vary the value of epsilon between 0.01 and 0.9\n",
    "- generate adversarial examples from the test set using this epsilon\n",
    "- plot the number of adversarial examples flagged\n",
    "- using the classifier for people/thing, plot the number of misclassified legit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "fe17bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9740",
   "metadata": {},
   "source": [
    "## 3.2 Adversarial training\n",
    "**Adversarial training** is a classical strategy to improve a model's robustness against attacks.  \n",
    "\n",
    "The main idea is to **train the model not only on legitimate examples but also on adversarially perturbed examples**. By doing this, the model learns to correctly classify even when inputs are slightly manipulated.\n",
    "\n",
    "In ART, this is implemented using the `AdversarialTrainer`, which automatically generates adversarial examples during training and includes them in each batch. This forces the model to adapt to adversarial perturbations and become more resistant to future attacks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5714fa",
   "metadata": {},
   "source": [
    "### Setup the classifiers\n",
    "- Retrieve the classifier used for people/things and make a copy *robust_classifier* (for comparison purposes)\n",
    "- Create a generator of advesarial examples based on PGD\n",
    "- Create a *AdversarialTrainer* using the copy and the attack model as argument\n",
    "- Launch a training with *fit* as usual => This is the actual adversarial training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "690bb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01a0b7",
   "metadata": {},
   "source": [
    "### Compare the accuracy of the resulting model with the accuracy of the original model\n",
    "Compute the accuracy of this classifier\n",
    "- on the test set\n",
    "- on adversarial examples generated using the previous attacker and a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "42dac805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6927a16",
   "metadata": {},
   "source": [
    "### Impact of epsilon\n",
    "Without retraining your robust model:\n",
    "- vary the value of epsilon between 0.01 and 0.9\n",
    "- generate adversarial examples from the test set and the original and robust classifier\n",
    "- plot the accuracy for both classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f0ad2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_tf_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
